{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Eleições 2022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indíce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando o Ambiente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependências \n",
    "try:\n",
    "    from google.colab import drive\n",
    "except:\n",
    "    pass\n",
    "from os import path, listdir, getcwd, mkdir\n",
    "from os.path import isfile, join\n",
    "import io\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variáveis Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_nome = None\n",
    "_workspace = None\n",
    "_env_mode = None\n",
    "_env = None\n",
    "_query = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_environment(workspace):\n",
    "    if path.exists(workspace + '/configuracao') == False:\n",
    "        mkdir(workspace + '/configuracao')\n",
    "        \n",
    "    if path.exists(workspace + '/Eleicoes_2022_Pesquisa') == False:\n",
    "        mkdir(workspace + '/Eleicoes_2022_Pesquisa')\n",
    "\n",
    "    if path.exists(workspace + '/Eleicoes_2022_Pesquisa/global') == False:\n",
    "        mkdir(workspace + '/Eleicoes_2022_Pesquisa/global')\n",
    "\n",
    "    if path.exists(workspace + '/Eleicoes_2022_Pesquisa/coleta') == False:\n",
    "        mkdir(workspace + '/Eleicoes_2022_Pesquisa/coleta')\n",
    "\n",
    "    if path.exists(workspace + '/Eleicoes_2022_Pesquisa/coleta/hashtags') == False:\n",
    "        mkdir(workspace + '/Eleicoes_2022_Pesquisa/coleta/hashtags')\n",
    "\n",
    "    if path.exists(workspace + '/Eleicoes_2022_Pesquisa/coleta/hashtags/configuracao') == False:\n",
    "        mkdir(workspace + '/Eleicoes_2022_Pesquisa/coleta/hashtags/configuracao')\n",
    "\n",
    "    if path.exists(workspace + '/Eleicoes_2022_Pesquisa/coleta/ego') == False:\n",
    "        mkdir(workspace + '/Eleicoes_2022_Pesquisa/coleta/ego')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_workspace():\n",
    "    global _env_mode\n",
    "    \n",
    "    workspace = \"\"\n",
    "    \n",
    "    if _env_mode == \"colab\":\n",
    "        try:\n",
    "            drive.mount('/content/drive')\n",
    "            #necessário executar caso queira pegar ou salvar os arquivos na pasta compartilhada\n",
    "        except:\n",
    "            print(\"Não foi possível montar o Drive.\")\n",
    "            raise Exception\n",
    "        else:\n",
    "            return '/drive/MyDrive/Colab Notebooks'\n",
    "\n",
    "    if _env_mode == \"root\":\n",
    "        if path.exists('/content') == False:\n",
    "            mkdir('/content')\n",
    "        workspace = '/content'\n",
    "\n",
    "    elif _env_mode == \"relative\":\n",
    "        workspace = \".\"\n",
    "        \n",
    "    else:\n",
    "        print(\"Diretórios não criados\")\n",
    "        raise Exception\n",
    "\n",
    "    build_environment(workspace)\n",
    "\n",
    "    return workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bruno\n",
    "#Função para carregamento de arquivos no drive \n",
    "def try_loadfrom_drive(loading_func):\n",
    "  global _workspace\n",
    "  \n",
    "  def wrapped(path, *args, **kwargs):\n",
    "    error_ignore = kwargs.pop('error_ignore', False)\n",
    "    \n",
    "    if path[0] !='/':\n",
    "        path = '/'+path\n",
    "  \n",
    "    if _env_mode == \"colab\":\n",
    "      try:\n",
    "        file = loading_func(_workspace + path, *args, **kwargs)\n",
    "      except:\n",
    "        if error_ignore:\n",
    "          file = None\n",
    "        else:\n",
    "          print(\"Recomendo criar um atalho no seu drive da pasta shareada\")\n",
    "          uploaded = files.upload()\n",
    "          upload_path = list(uploaded.keys())[0]\n",
    "          file = loading_func(io.BytesIO(uploaded[path]), *args, **kwargs)\n",
    "    else:\n",
    "      file = loading_func(_workspace + path, *args, **kwargs)\n",
    "    return file\n",
    "  return wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe para gerenciar variaveis salvas externamente\n",
    "#Cada usuário terá suas variáveis salvas no seu próprio workspace/google drive ou local\n",
    "#Cada usuário é identificado através da definição de uma variável \"nome\"\n",
    "#As variáveis locais ao usuário são salvas e carregadas de um arquivo, nomeado a partir do \"env_name\" selecionado\n",
    "#Quando uma variavel não é definida como privada, uma cópia é feita num arquivo global\n",
    "#Estas cópias podem ser acessadas através de uma referencia ao nome do detentor de uma variavel(e.g. Bruno)\n",
    "#Ou através da função get_globals()\n",
    "#Os arquivos de variaveis possuem o formato json/plain-text\n",
    "class Environment():\n",
    "  #Na instaciação de um objeto Env, o arquivo de variáveis \"{env_name}.json\" será pré-carregado para consulta.\n",
    "  def __init__(self, env_name):\n",
    "    self.path = env_name\n",
    "    env_file = try_loadfrom_drive(open)('/configuracao/'+env_name+'.json', 'r', error_ignore = True)\n",
    "    self.vars = json.load(env_file) if env_file else {}\n",
    "\n",
    "  #Função para recuperar uma variável com o nome {var}. Caso a variável ainda \n",
    "  #não exista para o usuário, será criada uma entrada nova a partir de um valor \n",
    "  #inicial.\n",
    "  #Caso o valor recuperado seja uma referência(e.g. \"$Bruno\"), um novo valor será recuperado \n",
    "  #do arquivo global, caso haja uma váriavel não privada definida para o usuário\n",
    "  #referenciado.\n",
    "\n",
    "  def update(self, g_id):\n",
    "    if path.exists(_workspace+'/configuracao') == False:\n",
    "          mkdir(_workspace+'/configuracao')\n",
    "\n",
    "    env_file = open(_workspace+'/configuracao/'+self.path+'.json', 'w')\n",
    "    json.dump(self.vars, env_file)\n",
    "    global_vars = {i:self.vars[i] for i in self.vars if not self.vars[i][1]}\n",
    "    env_file.close()\n",
    "\n",
    "    g_path = '/'+self.path+'/global/'+g_id+'.json'\n",
    "    if path.isfile(g_path):\n",
    "      global_env_file = try_loadfrom_drive(open)(g_path, 'r', error_ignore = True)\n",
    "      global_env_vars = json.load(global_env_file)\n",
    "      global_env_vars.update(global_vars)\n",
    "      global_env_file.close()\n",
    "\n",
    "    global_env_file = open(_workspace+g_path, 'w')\n",
    "    json.dump(global_vars, global_env_file)\n",
    "    global_env_file.close()\n",
    "\n",
    "  def set(self, var, value, is_private=None):\n",
    "    if not is_private:\n",
    "      try:\n",
    "        is_private = self.vars[var][1]\n",
    "      except:\n",
    "        is_private = False\n",
    "\n",
    "    self.vars.update({var:[value, is_private]})\n",
    "    self.update(value if var == 'nome' else _nome)\n",
    "\n",
    "  def get(self, var, generate_if_missing = True):\n",
    "    try:\n",
    "      value = self.vars[var][0]\n",
    "    except KeyError:\n",
    "      if not generate_if_missing:\n",
    "        return None\n",
    "      else:\n",
    "        value = input(f'Digite o valor inicial da variável de ambiente \"{var}\": ')\n",
    "        private = input('Variavel privada(True/False)?: ')\n",
    "        private = private == \"True\" or private == \"true\"\n",
    "        self.set(var, value, private)\n",
    "        \n",
    "    try:\n",
    "      if value[0] == '$':\n",
    "        linked_file = try_loadfrom_drive(open)('/'+self.path+'/global/'+value[1:]+'.json', 'r', error_ignore = True)\n",
    "        linked_vars = json.load(linked_file)\n",
    "        value = linked_vars[var][0]\n",
    "    except KeyError:\n",
    "      pass\n",
    "\n",
    "    return value\n",
    "\n",
    "  def delete(self, var):\n",
    "    self.vars.pop(var, None)\n",
    "    self.update(_nome)\n",
    "    \n",
    "  #Função helper vara retornar uma lista com todos os valores declarados de uma variável \n",
    "  #com o nome {var}, para todos os usuários.\n",
    "  #Não retorna variaveis privadas.\n",
    "  def get_globals(self, var):\n",
    "    g_path = _workspace+'/'+self.path+'/global'\n",
    "    global_files = [f for f in listdir(g_path) if isfile(join(g_path, f))]\n",
    "\n",
    "    values = []\n",
    "    for global_env in global_files:\n",
    "      if not _nome in global_env:\n",
    "        global_env_file = try_loadfrom_drive(open)('/'+self.path+'/global/'+global_env, 'r', error_ignore = True)\n",
    "        global_env_vars = json.load(global_env_file)\n",
    "        try:\n",
    "          values.append(global_env_vars[var][0])\n",
    "        except KeyError:\n",
    "          pass\n",
    "          \n",
    "        global_env_file.close()\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_environment(env_mode: str):\n",
    "    global _nome, _workspace, _env_mode, _env\n",
    "    \n",
    "    _env_mode = env_mode\n",
    "    if not _env_mode:\n",
    "        raise AttributeError\n",
    "    \n",
    "    _workspace = init_workspace()\n",
    "    #print(_workspace)\n",
    "    _env = Environment(\"Eleicoes_2022_Pesquisa\")\n",
    "    _nome = _env.get(\"nome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando Cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from http.client import RemoteDisconnected, HTTPException\n",
    "from time import sleep\n",
    "from urllib3 import exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variáveis Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_token_farm = None\n",
    "_client = {\n",
    "    \"coleta\": None,\n",
    "    \"superior\": None\n",
    "}\n",
    "_user_fields = None\n",
    "_tweet_fields = None\n",
    "_media_fields = None\n",
    "_expansions = None\n",
    "_dtype = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenFarmClient(tweepy.Client):\n",
    "  def __init__(\n",
    "        self, tokens, consumer_key=None, consumer_secret=None,\n",
    "        access_token=None, access_token_secret=None, *, return_type=tweepy.Response,\n",
    "        wait_on_rate_limit=False\n",
    "    ):\n",
    "    self.tokens = tokens\n",
    "    self.current_token = 0\n",
    "    super().__init__(bearer_token=tokens[0], consumer_key=consumer_key, \n",
    "                     consumer_secret=consumer_secret, access_token=access_token, \n",
    "                     access_token_secret=access_token_secret, return_type=return_type, \n",
    "                     wait_on_rate_limit=wait_on_rate_limit)\n",
    "\n",
    "  def request(\n",
    "          self, method, route, params=None, json=None, user_auth=False\n",
    "      ):\n",
    "    temp_wait_on_rate_limit = self.wait_on_rate_limit\n",
    "    self.wait_on_rate_limit = False\n",
    "    response_ok = False\n",
    "    try:\n",
    "      response = super().request(method, route, params, json, user_auth)\n",
    "      response_ok = True\n",
    "    except (tweepy.TooManyRequests, tweepy.Unauthorized):\n",
    "      for i in [x+self.current_token for x in range(len(self.tokens))]:\n",
    "        k = i % len(self.tokens)\n",
    "        self.bearer_token = self.tokens[k]\n",
    "        try:\n",
    "          response = super().request(method, route, params, json, user_auth)\n",
    "          response_ok = True\n",
    "          self.current_token = k\n",
    "          break\n",
    "        except (tweepy.TooManyRequests, tweepy.Unauthorized):\n",
    "          pass\n",
    "    \n",
    "    self.wait_on_rate_limit = temp_wait_on_rate_limit\n",
    "    if not response_ok:\n",
    "      self.current_token += 1\n",
    "      next = self.current_token\n",
    "      if next >= len(self.tokens):\n",
    "        self.current_token = next = 0\n",
    "      self.bearer_token = self.tokens[next]\n",
    "      response = super().request(method, route, params, json, user_auth)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_client():\n",
    "    global _token_farm\n",
    "\n",
    "    if _token_farm: \n",
    "         return tokenFarmClient(_env.get_globals(\"chave\"), wait_on_rate_limit = False)\n",
    "    else:\n",
    "        return tweepy.Client(_env.get(\"chave\"), wait_on_rate_limit= False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_client(thread = \"all\"):\n",
    "    global _client \n",
    "\n",
    "    if thread == \"all\":\n",
    "        _client.update({\n",
    "            \"coleta\": _init_client(),\n",
    "            \"superior\": _init_client(),\n",
    "            \"quotes\": _init_client(),\n",
    "            \"replies\": _init_client(),\n",
    "            \"likes\": _init_client(),\n",
    "            \"retweets\": _init_client()\n",
    "        })\n",
    "    \n",
    "    else:\n",
    "        _client[thread] = init_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execução\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tweet_fields = [\n",
    "    'created_at',\n",
    "    'public_metrics',\n",
    "    'text',\n",
    "    'id',\n",
    "    'conversation_id',\n",
    "    'entities',\n",
    "    'referenced_tweets',\n",
    "    'author_id',\n",
    "    'lang',\n",
    "    'source',\n",
    "    'in_reply_to_user_id',\n",
    "    'attachments'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_user_fields = [\n",
    "    'id',\n",
    "    'username',\n",
    "    'created_at',\n",
    "    'location',\n",
    "    'protected',\n",
    "    'public_metrics',\n",
    "    'verified',\n",
    "    'profile_image_url'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_media_fields = [\n",
    "    'type',\n",
    "    'url',\n",
    "    'public_metrics',\n",
    "    'preview_image_url'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_expansions = [\n",
    "    'author_id',\n",
    "    'attachments.media_keys'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dtype = {\n",
    "    'id': str,\n",
    "    'text': str,\n",
    "    'created_at': str,\n",
    "    'source': str,\n",
    "    'lang': str,\n",
    "    'conversation_id': str,\n",
    "    'like_count': int,\n",
    "    'retweet_count': int,\n",
    "    'quote_count': int,\n",
    "    'reply_count': int,\n",
    "    'type': str,\n",
    "    'referenced_tweet_id': str,\n",
    "    'mentions': str,\n",
    "    'hashtags': str,\n",
    "    'hastags': str,\n",
    "    'urls': str,\n",
    "    'author_id': str,\n",
    "    'media_keys': str\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coleta Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from math import nan\n",
    "from threading import Thread, Lock\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variáveis Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tweets_data = {\n",
    "    'coleta': None,\n",
    "    'superior': None,\n",
    "    'quotes': None,\n",
    "    'replies': None,\n",
    "    'likes': None,\n",
    "    'retweets': None\n",
    "}\n",
    "\n",
    "_users_data = {\n",
    "    'coleta': None,\n",
    "    'superior': None,\n",
    "    'quotes': None,\n",
    "    'replies': None,\n",
    "    'likes': None,\n",
    "    'retweets': None\n",
    "}\n",
    "\n",
    "_media_data = {\n",
    "    'coleta': None,\n",
    "    'superior': None,\n",
    "    'quotes': None,\n",
    "    'replies': None,\n",
    "    'likes': None,\n",
    "    'retweets': None\n",
    "}\n",
    "\n",
    "_backup_interval = None\n",
    "_env_mutex = Lock()\n",
    "_tweets_mutex = Lock()\n",
    "_users_mutex = Lock()\n",
    "_media_mutex = Lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __simplify_query(query: str) -> str:\n",
    "    new_query = query.replace('(', '')[0:30].replace('\"', '').replace(':', '_')\n",
    "    new_query = new_query.replace('ç', 'c').replace('ã', 'a')\n",
    "    return new_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __reset_data_frames(thread: str):\n",
    "    global _tweets_data, _users_data, _media_data, _tweets_mutex, _users_mutex, _media_mutex\n",
    "    \n",
    "    _tweets_mutex.acquire()\n",
    "    _tweets_data[thread] = pd.DataFrame(columns= ['id','text','created_at','source','lang','conversation_id','like_count','retweet_count','quote_count','reply_count','type','referenced_tweet_id','mentions','hashtags','urls','author_id','media_keys'])\n",
    "    _tweets_mutex.release()\n",
    "\n",
    "    _users_mutex.acquire()\n",
    "    _users_data[thread] = pd.DataFrame(columns= ['account_id','account_username','account_created_at','account_verified','account_protected','account_location','account_have_profile_image','account_followers_count','account_following_count','account_tweets_count'])\n",
    "    _users_mutex.release()\n",
    "\n",
    "    _media_mutex.acquire()\n",
    "    _media_data[thread] = pd.DataFrame(columns= ['media_key','media_type','media_url','media_view_count'])\n",
    "    _media_mutex.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __save_data_frames(csv_path: str, thread: str, *, sep=';', escapechar='\\\\', index = False, header= False, mode = 'a'):\n",
    "    global _tweets_data, _users_data, _media_data\n",
    "    \n",
    "    _tweets_mutex.acquire()\n",
    "    _tweets_data[thread].to_csv(csv_path + '_tweets.csv', sep= sep, escapechar= escapechar, index= index, header= header, mode= mode)\n",
    "    _tweets_mutex.release()\n",
    "\n",
    "    _users_mutex.acquire()\n",
    "    _users_data[thread].to_csv(csv_path + '_users.csv', sep= sep, escapechar= escapechar, index= index, header= header, mode= mode)\n",
    "    _users_mutex.release()\n",
    "\n",
    "    _media_mutex.acquire()\n",
    "    _media_data[thread].to_csv(csv_path + '_media.csv', sep= sep, escapechar= escapechar, index= index, header= header, mode= mode)\n",
    "    _media_mutex.release()\n",
    "\n",
    "    init_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_variables_coleta(query: str, query_name: str, date: datetime, thread: str):\n",
    "    global _env, _nome, _env_mutex\n",
    "\n",
    "    coleta_em_andamento = query_name + date.strftime(\"%d%m%Y\")\n",
    "\n",
    "    status_info = _env.get(coleta_em_andamento, generate_if_missing = False)\n",
    "    \n",
    "    year = date.strftime(\"%Y\")+'/'\n",
    "    month = date.strftime(\"%m\")+'/'\n",
    "    day = date.strftime(\"%d\")+'/'\n",
    "\n",
    "    coleta_query = _workspace + '/Eleicoes_2022_Pesquisa/coleta/hashtags/'+ query_name +'/'\n",
    "    coleta_year = coleta_query + year\n",
    "    coleta_month = coleta_year + month\n",
    "    coleta_day = coleta_month + day\n",
    "\n",
    "    if not path.exists(coleta_query):\n",
    "        mkdir(coleta_query)\n",
    "    if not path.exists(coleta_year):\n",
    "        mkdir(coleta_year)\n",
    "    if not path.exists(coleta_month):\n",
    "        mkdir(coleta_month)\n",
    "    if not path.exists(coleta_day):\n",
    "        mkdir(coleta_day)\n",
    "\n",
    "    if status_info:\n",
    "        if status_info['next_page_token'] == 'fim':\n",
    "            return False\n",
    "        _next_token = status_info['next_page_token']\n",
    "        # _total_collected = status_info['total_collected']\n",
    "        __reset_data_frames(thread)\n",
    "    \n",
    "    else:\n",
    "        coleta_data_id = datetime.now().strftime(\"%H%M%S%d%m%Y\")\n",
    "\n",
    "        status_info = {\n",
    "            \"csv_path\": coleta_day + _nome + \"_\" + date.strftime(\"%H%M%S%d%m%Y\") + \"_\" + __simplify_query(query) + \"_\" + coleta_data_id,\n",
    "            \"next_page_token\": None,\n",
    "            # \"total_collected\": 0,\n",
    "        }\n",
    "        \n",
    "        _env_mutex.acquire()\n",
    "        _env.set(coleta_em_andamento, status_info, True)\n",
    "        _env_mutex.release()\n",
    "\n",
    "        __reset_data_frames(thread)\n",
    "        __save_data_frames(status_info['csv_path'], thread, header= True)\n",
    "        \n",
    "        print(f\"thread {(thread.upper() + ':' + query_name):<24}{'START '+ str(date)}\")\n",
    "        \n",
    "        \n",
    "\n",
    "    return [coleta_em_andamento, status_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz o backup dos dados e salva o estado da coleta\n",
    "# input: indice do vetor de ids que a proxima coleta deve começar\n",
    "# output: nada\n",
    "def backup_state(coleta_em_andamento: str, status_info: dict, thread: str):\n",
    "    global _env, _env_mutex\n",
    "    print(f\"thread {(thread.upper()+':'):<12}{'BACKUP INICIADO'}\")\n",
    "\n",
    "    # Salva em csv\n",
    "    __save_data_frames(status_info['csv_path'], thread)\n",
    "\n",
    "    __reset_data_frames(thread)\n",
    "\n",
    "    _env_mutex.acquire()\n",
    "    _env.set(coleta_em_andamento, status_info, True)\n",
    "    _env_mutex.release()\n",
    "\n",
    "    print(f\"thread {(thread.upper()+':'):<12}{'BACKUP FINALIZADO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleciona os dados do objeto tweet e cooca em um dicionario\n",
    "# input: objeto tweet, vetor de tweets referenciados\n",
    "# output: dicionario de tweet\n",
    "def get_tweet_dict(tweet: tweepy.Tweet):\n",
    "    # Dados do tweet\n",
    "    tweet_dict = {\n",
    "        \"id\": tweet.id,\n",
    "        \"text\": tweet.text,\n",
    "        \"created_at\": str(tweet.created_at)[:-6],\n",
    "        \"source\": tweet.source,\n",
    "        \"lang\": tweet.lang,\n",
    "        \"conversation_id\": tweet.conversation_id,\n",
    "        \"like_count\": tweet.public_metrics[\"like_count\"],\n",
    "        \"retweet_count\": tweet.public_metrics[\"retweet_count\"],\n",
    "        \"quote_count\": tweet.public_metrics[\"quote_count\"],        # quote são retweets com comentário\n",
    "        \"reply_count\": tweet.public_metrics[\"reply_count\"],        # replies são respostas \n",
    "    }\n",
    "\n",
    "    if tweet.referenced_tweets != None:\n",
    "        type = []\n",
    "        referenced_tweet_id = [] \n",
    "        \n",
    "        for referenced_tweet in tweet.referenced_tweets:\n",
    "            type.append(referenced_tweet.type)\n",
    "            referenced_tweet_id.append(int(referenced_tweet.id))\n",
    "\n",
    "        tweet_dict.update({\n",
    "            \"type\": str(type)[1:-1].replace(\"'\",\"\"),\n",
    "            \"referenced_tweet_id\" : str(referenced_tweet_id)[1:-1].replace(\"'\",\"\")\n",
    "        })\n",
    "    else:\n",
    "        tweet_dict.update({\n",
    "            \"type\": \"tweeted\",\n",
    "            \"referenced_tweet_id\" : nan\n",
    "        })\n",
    "    \n",
    "    if tweet.entities != None:\n",
    "        # Menções \n",
    "        try:\n",
    "            mentions_ids = []\n",
    "\n",
    "            for mentions in  tweet.entities[\"mentions\"]:\n",
    "                mentions_ids.append(mentions[\"id\"])\n",
    "            \n",
    "            tweet_dict[\"mentions\"] = str(mentions_ids)[1:-1].replace(\"'\",\"\")\n",
    "            \n",
    "        except KeyError:\n",
    "            tweet_dict[\"mentions\"] = nan\n",
    "\n",
    "        # Hashtags\n",
    "\n",
    "        try:\n",
    "            hashtags_tags = []\n",
    "\n",
    "            for hashtags in tweet.entities[\"hashtags\"]:\n",
    "                hashtags_tags.append('#' + hashtags[\"tag\"])\n",
    "            \n",
    "            tweet_dict[\"hashtags\"] = str(hashtags_tags)[1:-1].replace(\"'\",\"\")\n",
    "            \n",
    "        except KeyError:\n",
    "            tweet_dict[\"hashtags\"] = nan\n",
    "\n",
    "        # Url\n",
    "\n",
    "        try:\n",
    "            urls = []\n",
    "\n",
    "            for url in tweet.entities[\"urls\"]:\n",
    "                urls.append(url[\"url\"])\n",
    "            \n",
    "            tweet_dict[\"urls\"] = str(urls)[1:-1].replace(\"'\",\"\")\n",
    "            \n",
    "        except KeyError:\n",
    "            tweet_dict[\"urls\"] = nan\n",
    "\n",
    "    # Dados de autor\n",
    "    tweet_dict[\"author_id\"] = tweet.author_id\n",
    "\n",
    "    # Dados de Media\n",
    "    media_keys = []\n",
    "\n",
    "    try:\n",
    "        for media_key in tweet.attachments['media_keys']:\n",
    "            media_keys.append(media_key)\n",
    "            \n",
    "        tweet_dict[\"media_keys\"] = str(media_keys)[1:-1].replace(\"'\",\"\")\n",
    "        \n",
    "    except:\n",
    "        tweet_dict[\"media_keys\"] = nan\n",
    "\n",
    "    return tweet_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleciona os dados do objeto user e cooca em um dicionario\n",
    "# input: objeto user\n",
    "# output: dicionario de user\n",
    "def get_user_dict(user: tweepy.User):\n",
    "    return {\n",
    "        'account_id': user.id,\n",
    "        'account_username': user.username,\n",
    "        'account_created_at': str(user.created_at)[:-6],\n",
    "        'account_verified': user.verified,\n",
    "        'account_protected': user.protected,\n",
    "        'account_location': user.location if user.location != '' else nan,\n",
    "        'account_have_profile_image': True if user.profile_image_url != '' else False,\n",
    "        'account_followers_count': user.public_metrics['followers_count'],\n",
    "        'account_following_count': user.public_metrics['following_count'], #followings + \n",
    "        'account_tweets_count': user.public_metrics['tweet_count'], #tweets + retweets da conta\n",
    "    }   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleciona os dados do objeto media e cooca em um dicionario\n",
    "# input: objeto media\n",
    "# output: dicionario de media\n",
    "def get_media_dict(media: tweepy.Media):\n",
    "    media_dict = {\n",
    "            \"media_key\": media.media_key,\n",
    "            \"media_type\": media.type\n",
    "    }\n",
    "                \n",
    "    if media.type == \"photo\":\n",
    "        media_dict.update({\n",
    "            \"media_url\": media.url,\n",
    "            \"media_view_count\": nan\n",
    "        })\n",
    "    else:\n",
    "        media_dict.update({\"media_url\": media.preview_image_url})\n",
    "\n",
    "        try:\n",
    "            media_dict[\"media_view_count\"] = media.public_metrics[\"view_count\"]\n",
    "        except TypeError:\n",
    "            media_dict[\"media_view_count\"] = nan\n",
    "\n",
    "    return media_dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response(response, thread: str):\n",
    "    global _tweets_data, _users_data, _media_data\n",
    "    \n",
    "    tweet_dict_list = []\n",
    "    users_dict_list = []\n",
    "    media_dict_list = []\n",
    "    \n",
    "    # Coleta os dicionarios dos dados\n",
    "    for tweet in response.data:\n",
    "        tweet_dict_list.append(get_tweet_dict(tweet))\n",
    "\n",
    "    for user in response.includes[\"users\"]:\n",
    "        users_dict_list.append(get_user_dict(user))\n",
    "\n",
    "    try:\n",
    "        for media in response.includes[\"media\"]:\n",
    "            media_dict_list.append(get_media_dict(media))\n",
    "    except KeyError as e:\n",
    "        pass\n",
    "\n",
    "    # Concatena os dados anteriormente coletados com os coletados\n",
    "    _tweets_mutex.acquire()\n",
    "    _tweets_data[thread] = pd.concat([_tweets_data[thread], pd.DataFrame(tweet_dict_list)], ignore_index = True)\n",
    "    _tweets_mutex.release()\n",
    "\n",
    "    _users_mutex.acquire()\n",
    "    _users_data[thread] = pd.concat([_users_data[thread], pd.DataFrame(users_dict_list)], ignore_index = True)\n",
    "    _users_mutex.release()\n",
    "\n",
    "    _media_mutex.acquire()\n",
    "    _media_data[thread] = pd.concat([_media_data[thread], pd.DataFrame(media_dict_list)], ignore_index = True)\n",
    "    _media_mutex.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_day_tweets_from_date(query: str, query_name: str, date: datetime, thread: str):\n",
    "    global _backup_interval\n",
    "    \n",
    "    backup = False\n",
    "    backup_time = time.time()\n",
    "\n",
    "    retorno = init_variables_coleta(query, query_name, date, thread)\n",
    "        \n",
    "    if retorno != False:\n",
    "\n",
    "        coleta_em_andamento = retorno[0]\n",
    "        status_info = retorno[1]\n",
    "        \n",
    "        del retorno\n",
    "        \n",
    "        while status_info[\"next_page_token\"] != \"fim\":\n",
    "            try:\n",
    "                response = _client[thread].search_all_tweets(\n",
    "                    query= query,\n",
    "                    user_fields= _user_fields,\n",
    "                    tweet_fields= _tweet_fields,\n",
    "                    media_fields= _media_fields,\n",
    "                    expansions= _expansions,\n",
    "                    max_results= 500,\n",
    "                    next_token= status_info[\"next_page_token\"],\n",
    "                    end_time= date,\n",
    "                    start_time= date - timedelta(days= 1)\n",
    "                )\n",
    "\n",
    "            except tweepy.TweepyException as e:\n",
    "                print(f\"thread {(thread.upper()+':'):<12}{e}\")\n",
    "            \n",
    "                if not backup:\n",
    "                    backup_state(coleta_em_andamento, status_info, thread)\n",
    "                    backup = True   \n",
    "                    \n",
    "                sleep(5)\n",
    "                \n",
    "            except KeyboardInterrupt as e: \n",
    "                print(f\"thread {(thread.upper()+':'):<12}{e}\")\n",
    "\n",
    "                if not backup:\n",
    "                    backup_state(coleta_em_andamento, status_info, thread)\n",
    "                    backup = True   \n",
    "                \n",
    "                raise KeyboardInterrupt\n",
    "\n",
    "            except (ConnectionError, RemoteDisconnected, exceptions.ProtocolError, HTTPException, ConnectionAbortedError, ConnectionRefusedError, ConnectionResetError, TimeoutError) as e:\n",
    "                print(f\"thread {(thread.upper()+':'):<12}{e}\")\n",
    "\n",
    "                if not backup:\n",
    "                    backup_state(coleta_em_andamento, status_info, thread)\n",
    "                    backup = True  \n",
    "\n",
    "                sleep(15)\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                # status_info[\"total_collected\"] += response.meta[\"result_count\"]\n",
    "                print(f\"thread {(thread.upper()+':'):<12}Coletados no total {response.meta['result_count']} tweets\")\n",
    "\n",
    "                if backup:\n",
    "                    backup_time = time.time()\n",
    "\n",
    "                try:\n",
    "                    status_info[\"next_page_token\"] = response.meta[\"next_token\"]\n",
    "                except KeyError:\n",
    "                    status_info[\"next_page_token\"] = \"fim\"\n",
    "\n",
    "                if(response.data != None):\n",
    "                    process_response(response, thread)\n",
    "\n",
    "                backup = False\n",
    "\n",
    "                if time.time() - backup_time > _backup_interval:\n",
    "                    backup_state(coleta_em_andamento, status_info, thread)\n",
    "                    backup = True\n",
    "        \n",
    "        backup_state(coleta_em_andamento, status_info, thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coleta(thread: str):\n",
    "    ordem = (\"query_bolsonaro\", \"query_lula\", \"query_ciro\", \"query_simone\", \"tweets_bolsonaro\", \"tweets_lula\", \"tweets_ciro\", \"tweets_simone\")\n",
    "    date = datetime(datetime.now().year, datetime.now().month, datetime.now().day)\n",
    "    today = date\n",
    "\n",
    "    while True:\n",
    "        while date > datetime(year= 2022, month= 9, day= 1):\n",
    "            for query in ordem:\n",
    "                #print(f\"thread {(thread.upper()+':'):<12}{query}\")\n",
    "                collect_day_tweets_from_date(_env.get(query), date - timedelta(days= 1), thread)\n",
    "\n",
    "            date -= timedelta(days= 1)\n",
    "\n",
    "            today = datetime(datetime.now().year, datetime.now().month, datetime.now().day)\n",
    "            if today > date:\n",
    "                date = today\n",
    "            else:\n",
    "                sleep(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coleta Rede Ativa Superior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_variables_superior(query:str, query_name: str, date: datetime, thread: str):\n",
    "    global _tweets_data, _users_data, _media_data, _env, _nome, _env_mutex\n",
    "\n",
    "    coleta_em_andamento = query_name + date.strftime(\"%d%m%Y\")\n",
    "\n",
    "    status_info = _env.get(coleta_em_andamento, generate_if_missing = False)\n",
    "    \n",
    "    if status_info == None:\n",
    "        return False\n",
    "\n",
    "    if status_info[\"next_page_token\"] != \"fim\":\n",
    "        return False\n",
    "    try:\n",
    "        if status_info[\"rede_ativa_superior\"]:\n",
    "            return False\n",
    "\n",
    "    except KeyError:\n",
    "        status_info.update({\"rede_ativa_superior\": False})\n",
    "        \n",
    "        _env_mutex.acquire()\n",
    "        _env.set(coleta_em_andamento, status_info, True)\n",
    "        _env_mutex.release()\n",
    "\n",
    "        status_info = _env.get(coleta_em_andamento, generate_if_missing = False)\n",
    "\n",
    "    __reset_data_frames(thread)\n",
    "    \n",
    "    print(f\"thread {(thread.upper() + ':' + query_name):<24}{'START '+ str(date)}\")\n",
    "\n",
    "    return [coleta_em_andamento, status_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_referenced_tweets(tweets_df: pd.DataFrame):\n",
    "    referenced_tweets_ids = {}\n",
    "    ids = {}\n",
    "    for line in tweets_df.index:\n",
    "        tweet = tweets_df.iloc[line]\n",
    "        \n",
    "        for referenced_tweet in str(tweet[\"referenced_tweet_id\"]).split(\", \"):\n",
    "            referenced_tweets_ids.update({str(referenced_tweet): True})\n",
    "        \n",
    "        ids.update({str(tweet['id']): True})\n",
    "\n",
    "    try:\n",
    "        referenced_tweets_ids.pop(\"nan\")\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        referenced_tweets_ids.pop(\"referenced_tweet_id\")\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    referenced_tweets_ids_list = list(referenced_tweets_ids.keys())\n",
    "\n",
    "    for id in referenced_tweets_ids_list:\n",
    "        try:\n",
    "            ids[id]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        else:\n",
    "            referenced_tweets_ids.pop(id)\n",
    "\n",
    "    return list(referenced_tweets_ids.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(tweets_ids: list, coleta_em_andamento: str, status_info: dict, thread: str):\n",
    "    global _backup_interval\n",
    "\n",
    "\n",
    "    backup = False\n",
    "    backup_time = time.time()\n",
    "\n",
    "    begin = 0\n",
    "    end = 0\n",
    "\n",
    "    begin = end\n",
    "    end += 100\n",
    "    \n",
    "    if end > len(tweets_ids):\n",
    "        end = len(tweets_ids)\n",
    "\n",
    "    # Divide a lista de ids em fatias de 100 ou menores\n",
    "    while end < len(tweets_ids):\n",
    "        \n",
    "        try:\n",
    "            response = _client[thread].get_tweets(\n",
    "                ids = tweets_ids[begin : end],\n",
    "                tweet_fields = _tweet_fields,\n",
    "                user_fields = _user_fields,\n",
    "                media_fields = _media_fields,\n",
    "                expansions = _expansions\n",
    "            )\n",
    "\n",
    "        except tweepy.TweepyException as e:\n",
    "            print(f\"thread {(thread.upper()+':'):<12}{e}\")\n",
    "            for i in tweets_ids:\n",
    "                print(i)\n",
    "            sleep(15)\n",
    "            \n",
    "        except KeyboardInterrupt as e: \n",
    "            print(f\"thread {(thread.upper()+':'):<12}{e}\") \n",
    "            \n",
    "            raise KeyboardInterrupt\n",
    "\n",
    "        except (ConnectionError, RemoteDisconnected, exceptions.ProtocolError, HTTPException) as e:\n",
    "            print(f\"thread {(thread.upper()+':'):<12}{e}\")\n",
    "\n",
    "            sleep(15)\n",
    "            \n",
    "        else:\n",
    "            print(f\"thread {(thread.upper()+':'):<12}{f'Coletados {(end/len(tweets_ids)*100):.2f}% tweets relacionados'}\")\n",
    "            \n",
    "            process_response(response, thread)\n",
    "\n",
    "            # Garante as fatias\n",
    "            begin = end\n",
    "            end += 100\n",
    "            \n",
    "            if end > len(tweets_ids):\n",
    "                end = len(tweets_ids)\n",
    "\n",
    "            \n",
    "    status_info[\"rede_ativa_superior\"] = True\n",
    "    backup_state(coleta_em_andamento, status_info, thread)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rede_ativa_superior(query: str, query_name: str, date: datetime, thread: str):\n",
    "    retorno = init_variables_superior(query, query_name, date, thread)\n",
    "    \n",
    "    if retorno != False:\n",
    "        coleta_em_andamento = retorno[0]\n",
    "        status_info = retorno[1]\n",
    "\n",
    "        del retorno\n",
    "\n",
    "        if status_info['csv_path'] == False:\n",
    "            return False\n",
    "\n",
    "        referenced_tweets = get_referenced_tweets(pd.read_csv(status_info['csv_path'] + '_tweets.csv', sep=';', escapechar= '\\\\', dtype= _dtype, on_bad_lines= \"warn\"))\n",
    "        #referenced_tweets = get_referenced_tweets(pd.read_csv(status_info['csv_path'] + '_tweets.csv', sep=';'))\n",
    "\n",
    "        get_tweets(referenced_tweets, coleta_em_andamento, status_info, thread)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superior(thread: str):\n",
    "    ordem = (\"query_bolsonaro\", \"query_lula\", \"query_ciro\", \"query_simone\", \"tweets_bolsonaro\", \"tweets_lula\", \"tweets_ciro\", \"tweets_simone\")\n",
    "    date = datetime(datetime.now().year, datetime.now().month, datetime.now().day)\n",
    "    today = date\n",
    "\n",
    "    while True:\n",
    "        while date > datetime(year= 2022, month= 9, day= 1):\n",
    "            for query in ordem:\n",
    "                print(f\"thread {(thread.upper()+':'):<12}{query}\")\n",
    "                get_rede_ativa_superior(_env.get(query), date - timedelta(days= 1), thread)\n",
    "\n",
    "            date -= timedelta(days= 1)\n",
    "\n",
    "            today = datetime(datetime.now().year, datetime.now().month, datetime.now().day)\n",
    "            if today > date:\n",
    "                date = today\n",
    "            else:\n",
    "                sleep(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coleta Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_variables_interactions(query: str, query_name: str, date: datetime, thread: str):\n",
    "    global _env, _nome, _env_mutex\n",
    "\n",
    "    coleta_em_andamento = query_name + date.strftime(\"%d%m%Y\")\n",
    "\n",
    "    status_info = _env.get(coleta_em_andamento, generate_if_missing = False)\n",
    "\n",
    "    directory = _workspace + '/Eleicoes_2022_Pesquisa/coleta/hashtags/'+ query_name + '/'\n",
    "    directory += date.strftime(\"%Y/%m/%d/\") + thread\n",
    "\n",
    "    if not path.exists(directory):\n",
    "        mkdir(directory)\n",
    "    \n",
    "\n",
    "    # coleta não iniciada\n",
    "    if status_info == None:\n",
    "        return False\n",
    "\n",
    "    # coleta não finalizada\n",
    "    if status_info[\"next_page_token\"] != \"fim\":\n",
    "        return False\n",
    "\n",
    "\n",
    "    try:\n",
    "        # rede suaperior não finalizada\n",
    "        if not status_info[\"rede_ativa_superior\"]:\n",
    "            return False\n",
    "\n",
    "    # rede superior não iniciada\n",
    "    except KeyError:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        # quotes não finalizada\n",
    "        if status_info[thread][\"index\"] == \"fim\":\n",
    "            return False\n",
    "        else:\n",
    "            [coleta_em_andamento, status_info]\n",
    "            __reset_data_frames(thread)\n",
    "    \n",
    "    # quotes não iniciada\n",
    "    except KeyError:\n",
    "\n",
    "        status_info.update({\n",
    "            thread: {\n",
    "                \"index\": 0,\n",
    "                \"next_page_token\": None,\n",
    "                # \"total_collected\": 0,\n",
    "            }\n",
    "        })\n",
    "\n",
    "        __reset_data_frames(thread)\n",
    "\n",
    "        csv_path = status_info['csv_path']\n",
    "        __save_data_frames((csv_path[:csv_path.rfind('/')+1] + '/'+ thread +'/' + thread), thread, header= True)\n",
    "\n",
    "    _env_mutex.acquire()\n",
    "    _env.set(coleta_em_andamento, status_info, True)\n",
    "    _env_mutex.release()\n",
    "    \n",
    "    print(f\"thread {(thread.upper() + ':' + query_name):<24}{'START '+ str(date)}\")\n",
    "\n",
    "    return [coleta_em_andamento, status_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz o backup dos dados e salva o estado da coleta\n",
    "# input: indice do vetor de ids que a proxima coleta deve começar\n",
    "# output: nada\n",
    "def backup_interactions(coleta_em_andamento: str, status_info: dict, thread: str):\n",
    "    global _users_data, _env, _env_mutex, _users_mutex\n",
    "    #print(f\"thread {(thread.upper()+':'):<12}{'BACKUP INICIADO'}\")\n",
    "\n",
    "    # Salva em csv\n",
    "    csv_path = status_info['csv_path']\n",
    "    __save_data_frames((csv_path[:csv_path.rfind('/')+1] + '/'+ thread +'/' + thread), thread)\n",
    "    __reset_data_frames(thread)\n",
    "\n",
    "    _env_mutex.acquire()\n",
    "    _env.set(coleta_em_andamento, status_info, True)\n",
    "    _env_mutex.release()\n",
    "\n",
    "    #print(f\"thread {(thread.upper()+':'):<12}{'BACKUP FINALIZADO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes(query: str, query_name: str, date: datetime, thread: str):\n",
    "    retorno = init_variables_interactions(query, query_name, date, thread)\n",
    "\n",
    "    if retorno != False:\n",
    "        coleta_em_andamento = retorno[0]\n",
    "        status_info = retorno[1]\n",
    "\n",
    "        pd.DataFrame(columns=[\"account_id\", \"tweet_id\", \"interaction_authors\", \"interaction_ids\"]).to_csv(status_info[\"csv_path\"] + \"_\"+thread+\".csv\", sep=';', index = False, header= True, mode = 'a')\n",
    "\n",
    "        finished = False\n",
    "        while not finished: \n",
    "\n",
    "            tweet = pd.read_csv(status_info[\"csv_path\"] + \"_tweets.csv\", sep=\";\", escapechar= '\\\\', dtype= _dtype, skiprows= range(1,status_info[thread][\"index\"]), nrows=1)\n",
    "\n",
    "            finished = len(tweet.index) == 0\n",
    "            if not finished:\n",
    "\n",
    "                if str(tweet.at[0,\"type\"])!= \"retweeted\" and (int(tweet.at[0,\"quote_count\"]) > 0 if str(tweet.at[0,\"type\"])!= \"type\" else False):\n",
    "                    print(f\"thread {(thread.upper()+':'):<12}linha {status_info[thread]['index']}\")\n",
    "                    \n",
    "                    local_query = \"quotes_of_tweet_id:\" + str(tweet.at[0,\"id\"]) + \" lang:pt\"\n",
    "                    retorno = get_interactions_tweets(local_query, coleta_em_andamento, status_info, thread)\n",
    "\n",
    "                    if retorno != False:\n",
    "                        pd.DataFrame(\n",
    "                            [{\n",
    "                                \"account_id\": str(tweet.at[0,\"author_id\"]),\n",
    "                                \"tweet_id\": str(tweet.at[0,\"id\"]),\n",
    "                                \"interaction_authors\": retorno[1],\n",
    "                                \"interaction_ids\": retorno[0]\n",
    "                            }]\n",
    "                        ).to_csv(status_info[\"csv_path\"] + \"_quotes.csv\", sep=';', index = False, header= False, mode = 'a')\n",
    "                        status_info = retorno[2]\n",
    "\n",
    "                status_info[thread][\"index\"] += 1\n",
    "\n",
    "                backup_interactions(coleta_em_andamento, status_info, thread)\n",
    "        \n",
    "        status_info[thread][\"index\"] = \"fim\"\n",
    "        backup_interactions(coleta_em_andamento, status_info, thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interactions_tweets(query: str, coleta_em_andamento: str, status_info: dict, thread: str):\n",
    "    \n",
    "    while status_info[thread][\"next_page_token\"] != \"fim\":\n",
    "        try:\n",
    "            response = _client[thread].search_all_tweets(\n",
    "                query= query,\n",
    "                user_fields= _user_fields,\n",
    "                tweet_fields= _tweet_fields,\n",
    "                media_fields= _media_fields,\n",
    "                expansions= _expansions,\n",
    "                max_results= 500,\n",
    "                next_token= status_info[thread][\"next_page_token\"],\n",
    "                start_time= datetime(year= 2022, month= 7, day= 21)\n",
    "            )\n",
    "\n",
    "        except tweepy.TweepyException as e:\n",
    "            print(f\"thread {(thread.upper()+':'):<12}{e}\")\n",
    "            sleep(15)\n",
    "            \n",
    "        except KeyboardInterrupt as e: \n",
    "            print(f\"thread {(thread.upper()+':'):<12}{e}\")\n",
    "            raise KeyboardInterrupt\n",
    "\n",
    "        except (ConnectionError, RemoteDisconnected, exceptions.ProtocolError, HTTPException) as e:\n",
    "            print(f\"thread {(thread.upper()+':'):<12}{e}\")\n",
    "            sleep(15)\n",
    "        else:\n",
    "\n",
    "            # status_info[thread][\"total_collected\"] += response.meta[\"result_count\"]\n",
    "            print(f\"thread {(thread.upper()+':'):<12}Coletados no total {response.meta['result_count']} tweets\")\n",
    "            \n",
    "            try:\n",
    "                status_info[thread][\"next_page_token\"] = response.meta[\"next_token\"]\n",
    "            except KeyError:\n",
    "                status_info[thread][\"next_page_token\"] = \"fim\"\n",
    "\n",
    "            if response.data == None:\n",
    "                status_info[thread][\"next_page_token\"] = None\n",
    "                return False\n",
    "                \n",
    "            process_response(response, thread)\n",
    "    \n",
    "    status_info[thread][\"next_page_token\"] = None\n",
    "\n",
    "    return [str(list(_tweets_data[thread][\"id\"]))[1:-1], str(list(_tweets_data[thread][\"author_id\"]))[1:-1], status_info]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coleta Replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replies(query: str, query_name:str, date: datetime, thread: str):\n",
    "    retorno = init_variables_interactions(query, query_name, date, thread)\n",
    "\n",
    "    if retorno != False:\n",
    "        coleta_em_andamento = retorno[0]\n",
    "        status_info = retorno[1]\n",
    "\n",
    "        pd.DataFrame(columns=[\"account_id\", \"tweet_id\", \"interaction_authors\", \"interaction_ids\"]).to_csv(status_info[\"csv_path\"] + \"_\"+thread+\".csv\", sep=';', index = False, header= True, mode = 'a')\n",
    "\n",
    "        finished = False\n",
    "        conversation_hash = {}\n",
    "\n",
    "        while not finished: \n",
    "           \n",
    "            tweet = pd.read_csv(status_info[\"csv_path\"] + \"_tweets.csv\", sep=\";\", escapechar= '\\\\', dtype= _dtype, skiprows= range(1,status_info[thread][\"index\"]), nrows=1)\n",
    "\n",
    "            finished = len(tweet.index) == 0\n",
    "            if not finished:\n",
    "                if str(tweet.at[0,\"type\"])!= \"retweeted\" and (int(tweet.at[0,\"reply_count\"]) > 0 if str(tweet.at[0,\"type\"])!= \"type\" else False):\n",
    "                    try:\n",
    "                        conversation_hash[str(tweet.at[0,\"conversation_id\"])]\n",
    "                    \n",
    "                    except KeyError:\n",
    "                        conversation_hash.update({str(tweet.at[0,\"conversation_id\"]): True}) \n",
    "                    \n",
    "                        print(f\"thread {(thread.upper()+':'):<12}linha {status_info[thread]['index']}\")\n",
    "                        local_query = \"conversation_id:\" + str(tweet.at[0,\"conversation_id\"]) + \" lang:pt\"\n",
    "                        retorno = get_interactions_tweets(local_query, coleta_em_andamento, status_info, thread)\n",
    "                \n",
    "                        if retorno != False:\n",
    "                            pd.DataFrame(\n",
    "                                [{\n",
    "                                    \"account_id\": str(tweet.at[0,\"author_id\"]),\n",
    "                                    \"tweet_id\": str(tweet.at[0,\"id\"]),\n",
    "                                    \"interaction_authors\": retorno[1],\n",
    "                                    \"interaction_ids\": retorno[0]\n",
    "                                }]\n",
    "                            ).to_csv(status_info[\"csv_path\"] + \"_replies.csv\", sep=';', index = False, header= False, mode = 'a')\n",
    "                            status_info = retorno[2]\n",
    "\n",
    "            status_info[thread][\"index\"] += 1\n",
    "\n",
    "            backup_interactions(coleta_em_andamento, status_info, thread)\n",
    "        \n",
    "        status_info[thread][\"index\"] = \"fim\"\n",
    "        backup_interactions(coleta_em_andamento, status_info, thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coleta Likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response_intercations(response, thread: str):\n",
    "    global _users_data\n",
    "    \n",
    "    users_dict_list = []\n",
    "\n",
    "    # Coleta os dicionarios dos dados\n",
    "    for user in response.data:\n",
    "        users_dict_list.append(get_user_dict(user))\n",
    "\n",
    "    # Concatena os dados anteriormente coletados com os coletados \n",
    "\n",
    "    _users_mutex.acquire()\n",
    "    _users_data[thread] = pd.concat([_users_data[thread], pd.DataFrame(users_dict_list)], ignore_index = True)\n",
    "    _users_mutex.release()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interactions(function, id: str, coleta_em_andamento: str, status_info: dict, thread: str):\n",
    "    \n",
    "    while status_info[thread][\"next_page_token\"] != \"fim\":\n",
    "        try:\n",
    "            response = function(\n",
    "                id= id,\n",
    "                user_fields= _user_fields,\n",
    "                max_results= 100,\n",
    "                pagination_token= status_info[thread][\"next_page_token\"],\n",
    "            )\n",
    "\n",
    "        except tweepy.TweepyException as e:\n",
    "            print(f\"thread {(thread.upper()+':'):<12}{e}\")\n",
    "            sleep(15)\n",
    "            \n",
    "        except KeyboardInterrupt as e: \n",
    "            print(f\"thread {(thread.upper()+':'):<12}{e}\")\n",
    "            raise KeyboardInterrupt\n",
    "\n",
    "        except (ConnectionError, RemoteDisconnected, exceptions.ProtocolError, HTTPException) as e:\n",
    "            print(f\"thread {(thread.upper()+':'):<12}{e}\")\n",
    "            sleep(15)\n",
    "        else:\n",
    "            if response.data == None:\n",
    "                status_info[thread][\"next_page_token\"] = None\n",
    "                return False\n",
    "            \n",
    "            # status_info[thread][\"total_collected\"] += response.meta[\"result_count\"]\n",
    "            print(f\"thread {(thread.upper()+':'):<12}Coletados no total {response.meta['result_count']} tweets\")\n",
    "        \n",
    "            try:\n",
    "                status_info[thread][\"next_page_token\"] = response.meta[\"next_token\"]\n",
    "            except KeyError:\n",
    "                status_info[thread][\"next_page_token\"] = \"fim\"\n",
    "\n",
    "            \n",
    "            process_response_intercations(response, thread)\n",
    "\n",
    "    status_info[thread][\"next_page_token\"] = None\n",
    "\n",
    "    return [str(list(_users_data[thread][\"account_id\"]))[1:-1], status_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likes(query: str, date: datetime, thread: str):\n",
    "    retorno = init_variables_interactions(query, date, thread)\n",
    "\n",
    "    if retorno != False:\n",
    "        coleta_em_andamento = retorno[0]\n",
    "        status_info = retorno[1]\n",
    "\n",
    "        pd.DataFrame(columns=[\"account_id\", \"tweet_id\", \"interaction_authors\"]).to_csv(status_info[\"csv_path\"] + \"_\"+thread+\".csv\", sep=';', index = False, header= True, mode = 'a')\n",
    "\n",
    "        finished = False\n",
    "        while not finished: \n",
    "            tweet = pd.read_csv(status_info[\"csv_path\"] + \"_tweets.csv\", sep=\";\", escapechar= '\\\\', dtype= _dtype, skiprows= range(1, status_info[thread][\"index\"]), nrows=1)\n",
    "\n",
    "            finished = len(tweet.index) == 0\n",
    "            if not finished:\n",
    "                if str(tweet.at[0,\"type\"])!= \"retweeted\" and (int(tweet.at[0,'like_count']) > 0 if str(tweet.at[0,\"type\"])!= \"type\" else False):\n",
    "                    print(f\"thread {(thread.upper()+':'):<12}linha {status_info[thread]['index']}\")\n",
    "                    retorno = get_interactions(_client[thread].get_liking_users, str(tweet.at[0,\"id\"]), coleta_em_andamento, status_info, thread)\n",
    "\n",
    "                    if retorno != False:\n",
    "                        pd.DataFrame(\n",
    "                            [{\n",
    "                                \"account_id\": str(tweet.at[0,\"author_id\"]),\n",
    "                                \"tweet_id\": str(tweet.at[0,\"id\"]),\n",
    "                                \"interaction_authors\": retorno[0],\n",
    "                            }]\n",
    "                        ).to_csv(status_info[\"csv_path\"] + \"_likes.csv\", sep=';', index = False, header= False, mode = 'a')\n",
    "                        status_info = retorno[1]\n",
    "\n",
    "            status_info[thread][\"index\"] += 1\n",
    "\n",
    "            backup_interactions(coleta_em_andamento, status_info, thread)\n",
    "        \n",
    "        status_info[thread][\"index\"] = \"fim\"\n",
    "        backup_interactions(coleta_em_andamento, status_info, thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retweets(query: str, date: datetime, thread: str):\n",
    "    retorno = init_variables_interactions(query, date, thread)\n",
    "\n",
    "    if retorno != False:\n",
    "        coleta_em_andamento = retorno[0]\n",
    "        status_info = retorno[1]\n",
    "\n",
    "        pd.DataFrame(columns=[\"account_id\", \"tweet_id\", \"interaction_authors\"]).to_csv(status_info[\"csv_path\"] + \"_\"+thread+\".csv\", sep=';', index = False, header= True, mode = 'a')\n",
    "\n",
    "        finished = False\n",
    "        while not finished: \n",
    "            tweet = pd.read_csv(status_info[\"csv_path\"] + \"_tweets.csv\", sep=\";\", escapechar= '\\\\', dtype= _dtype, skiprows= range(1, status_info[thread][\"index\"]), nrows=1)\n",
    "\n",
    "            finished = len(tweet.index) == 0\n",
    "            if not finished:\n",
    "                if str(tweet.at[0,\"type\"])!= \"retweeted\" and (int(tweet.at[0,\"retweet_count\"]) > 0 if str(tweet.at[0,\"type\"])!= \"type\" else False):\n",
    "                    print(f\"thread {(thread.upper()+':'):<12}linha {status_info[thread]['index']}\")\n",
    "                    retorno = get_interactions(_client[thread].get_retweeters, str(tweet.at[0,\"id\"]), coleta_em_andamento, status_info, thread)\n",
    "                \n",
    "                    if retorno != False:\n",
    "                        pd.DataFrame(\n",
    "                            [{\n",
    "                                \"account_id\": str(tweet.at[0,\"author_id\"]),\n",
    "                                \"tweet_id\": str(tweet.at[0,\"id\"]),\n",
    "                                \"interaction_authors\": retorno[0],\n",
    "                            }]\n",
    "                        ).to_csv(status_info[\"csv_path\"] + \"_retweets.csv\", sep=';', index = False, header= False, mode = 'a')\n",
    "                        status_info = retorno[1]\n",
    "\n",
    "            status_info[thread][\"index\"] += 1\n",
    "\n",
    "            backup_interactions(coleta_em_andamento, status_info, thread)\n",
    "        \n",
    "        status_info[thread][\"index\"] = \"fim\"\n",
    "        backup_interactions(coleta_em_andamento, status_info, thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retweets2(query: str, date: datetime, thread: str):\n",
    "    retorno = init_variables_interactions(query, date, thread)\n",
    "\n",
    "    if retorno != False:\n",
    "        coleta_em_andamento = retorno[0]\n",
    "        status_info = retorno[1]\n",
    "\n",
    "        pd.DataFrame(columns=[\"account_id\", \"tweet_id\", \"interaction_authors\", \"interaction_ids\"]).to_csv(status_info[\"csv_path\"] + \"_\"+thread+\".csv\", sep=';', index = False, header= True, mode = 'a')\n",
    "\n",
    "        finished = False\n",
    "        while not finished: \n",
    "\n",
    "            tweet = pd.read_csv(status_info[\"csv_path\"] + \"_tweets.csv\", sep=\";\", escapechar= '\\\\', dtype= _dtype, skiprows= range(1,status_info[thread][\"index\"]), nrows=1)\n",
    "            finished = len(tweet.index) == 0\n",
    "            if not finished:\n",
    "\n",
    "                if str(tweet.at[0,\"type\"])!= \"retweeted\" and (int(tweet.at[0,\"retweet_count\"]) > 0 if str(tweet.at[0,\"type\"])!= \"type\" else False):\n",
    "                    print(f\"thread {(thread.upper()+':'):<12}linha {status_info[thread]['index']}\")\n",
    "                    \n",
    "                    local_query = \"retweets_of_tweet_id:\" + str(tweet.at[0,\"id\"]) + \" lang:pt\"\n",
    "                    retorno = get_interactions_tweets(local_query, coleta_em_andamento, status_info, thread)\n",
    "\n",
    "                    if retorno != False:\n",
    "                        pd.DataFrame(\n",
    "                            [{\n",
    "                                \"account_id\": str(tweet.at[0,\"author_id\"]),\n",
    "                                \"tweet_id\": str(tweet.at[0,\"id\"]),\n",
    "                                \"interaction_authors\": retorno[1],\n",
    "                                \"interaction_ids\": retorno[0]\n",
    "                            }]\n",
    "                        ).to_csv(status_info[\"csv_path\"] + \"_retweets.csv\", sep=';', index = False, header= False, mode = 'a')\n",
    "                        status_info = retorno[2]\n",
    "\n",
    "                status_info[thread][\"index\"] += 1\n",
    "\n",
    "                backup_interactions(coleta_em_andamento, status_info, thread)\n",
    "\n",
    "        status_info[thread][\"index\"] = \"fim\"\n",
    "        backup_interactions(coleta_em_andamento, status_info, thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retweets3(query: str, date: datetime, thread: str):\n",
    "    retorno = init_variables_interactions(query, date, thread)\n",
    "\n",
    "    if retorno != False:\n",
    "        coleta_em_andamento = retorno[0]\n",
    "        status_info = retorno[1]\n",
    "\n",
    "        pd.DataFrame(columns=[\"account_id\", \"tweet_id\", \"interaction_authors\", \"interaction_ids\"]).to_csv(status_info[\"csv_path\"] + \"_\"+thread+\".csv\", sep=';', index = False, header= True, mode = 'a')\n",
    "\n",
    "        tweets = pd.read_csv(status_info[\"csv_path\"] + \"_tweets.csv\", sep=\";\", escapechar= '\\\\', dtype= _dtype)\n",
    "\n",
    "        finished = False\n",
    "        while not finished: \n",
    "            tweet = tweets.iloc[status_info[thread][\"index\"], :]\n",
    "\n",
    "            finished = len(tweets.index) < status_info[thread][\"index\"] \n",
    "\n",
    "            if not finished:\n",
    "\n",
    "                if str(tweet.type)!= \"retweeted\" and (int(tweet.retweet_count) > 0 if str(tweet.type)!= \"type\" else False):\n",
    "                    print(f\"thread {(thread.upper()+':'):<12}linha {status_info[thread]['index']}\")\n",
    "                    \n",
    "                    local_query = \"retweets_of_tweet_id:\" + str(tweet.id) + \" lang:pt\"\n",
    "                    retorno = get_interactions_tweets(local_query, coleta_em_andamento, status_info, thread)\n",
    "\n",
    "                    if retorno != False:\n",
    "                        pd.DataFrame(\n",
    "                            [{\n",
    "                                \"account_id\": str(tweet.author_id),\n",
    "                                \"tweet_id\": str(tweet.id),\n",
    "                                \"interaction_authors\": retorno[1],\n",
    "                                \"interaction_ids\": retorno[0]\n",
    "                            }]\n",
    "                        ).to_csv(status_info[\"csv_path\"] + \"_retweets.csv\", sep=';', index = False, header= False, mode = 'a')\n",
    "                        status_info = retorno[2]\n",
    "\n",
    "                status_info[thread][\"index\"] += 1\n",
    "\n",
    "                backup_interactions(coleta_em_andamento, status_info, thread)\n",
    "        \n",
    "        status_info[thread][\"index\"] = \"fim\"\n",
    "        backup_interactions(coleta_em_andamento, status_info, thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_environment(\"relative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "_token_farm = True\n",
    "init_client()\n",
    "\n",
    "_backup_interval = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"jair OR bolsonaro OR bozo OR biroliro OR bonoro OR tchutchuca do centrao OR capitao OR genocida OR mito OR bolsomito OR bolsolixo OR bolsotrump OR messias OR patriota OR b22 OR b17 OR forabolsonaro OR #elesim OR #elenao OR mentiroso da republica OR #bolsonaronoprimeiroturno lang: pt\"\n",
    "\n",
    "_env.set(\"query_bolsonaro\", '(#bolsonaro OR jair OR bolsonaro OR bozo OR biroliro OR \"tchutchuca do centrao\" OR bonoro OR capitao OR genocida OR mito OR bolsomito OR bolsolixo OR bolsotrump OR messias OR patriota OR b22 OR b17 OR brocha OR imbrochavel OR maçonaro) lang:pt -is:retweet', False)\n",
    "\n",
    "_env.set(\"query_lula\",'(#lula OR lula OR \"ex presidiario\" OR lulalivre OR \"9 dedos\" OR luladrao OR lulaladrao OR lulinha OR nine OR luis inacio OR cachaceiro OR \"sapo barbudo\" OR lulao OR l13 OR \"faz o L\" OR lulindo OR metalurgico OR lulalkimin) -loud lang:pt -is:retweet', False)\n",
    "\n",
    "_env.set(\"query_ciro\", '(#ciro OR ciro OR c12 OR cirogomes OR \"ciro gomes\" OR \"correu pra paris\" OR bolsolula OR ciranha) lang:pt -is:retweet', False)\n",
    "\n",
    "_env.set(\"query_simone\", '(#simone OR \"simone tebet\" OR simonetebet OR tebet OR \"simone tablet\" OR estepe OR s15) lang:pt -is:retweet', False)\n",
    "\n",
    "_env.set(\"perfil_bolsonaro\", 'from:jairbolsonaro lang:pt', False)\n",
    "\n",
    "_env.set(\"perfil_lula\",'from:LulaOficial lang:pt', False)\n",
    "\n",
    "_env.set(\"perfil_ciro\", 'from:cirogomes lang:pt', False)\n",
    "\n",
    "_env.set(\"perfil_simone\", 'from:simonetebetbr lang:pt', False)\n",
    "\n",
    "_env.set(\"numero_bolsonaro\", '22 lang:pt -is:retweet', False)\n",
    "\n",
    "_env.set(\"numero_lula\",'13 lang:pt -is:retweet', False)\n",
    "\n",
    "_env.set(\"pos_eleicao\", '(\"intervenção militar\" OR \"intervenção federal\" OR \"alexandre de morais\" OR xandão OR fraude OR Venezuela OR Cuba OR urna OR urnas OR comunismo) lang:pt -is:retweet', False)\n",
    "\n",
    "_env.set(\"atos_golpistas\", '(\"Festa da Selma\" OR ato OR bolsonarista OR golpe OR golpista OR baderna OR extremista OR Brasília OR \"três poderes\" OR invasão OR \"ocupar congresso\" OR \"atos terroristas\" OR manifestção OR atentado OR patriotas OR \"tomada do poder\" OR guerra OR \"esplanada dos ministérios\" OR \"congresso nacional\" OR \"manifestantes\" OR \"retomada do poder\") -ucrania lang:pt -is:retweet')\n",
    "\n",
    "_env.set(\"retweets_query_bolsonaro\", 'is:retweet (#bolsonaro OR jair OR bolsonaro OR bozo OR biroliro OR \"tchutchuca do centrao\" OR bonoro OR capitao OR genocida OR mito OR bolsomito OR bolsolixo OR bolsotrump OR messias OR patriota OR b22 OR b17 OR brocha OR imbrochavel OR maçonaro) lang:pt ', False)\n",
    "\n",
    "_env.set(\"retweets_query_lula\",'is:retweet (#lula OR lula OR \"ex presidiario\" OR lulalivre OR \"9 dedos\" OR luladrao OR lulaladrao OR lulinha OR nine OR luis inacio OR cachaceiro OR \"sapo barbudo\" OR lulao OR l13 OR \"faz o L\" OR lulindo OR metalurgico OR lulalkimin) -loud lang:pt ', False)\n",
    "\n",
    "_env.set(\"retweets_query_ciro\", 'is:retweet (#ciro OR ciro OR c12 OR cirogomes OR \"ciro gomes\" OR \"correu pra paris\" OR bolsolula OR ciranha) lang:pt', False)\n",
    "\n",
    "_env.set(\"retweets_query_simone\", 'is:retweet (#simone OR \"simone tebet\" OR simonetebet OR tebet OR \"simone tablet\" OR estepe OR s15) lang:pt', False)\n",
    "\n",
    "_env.set(\"retweets_atos_golpistas\", 'is:retweet (\"Festa da Selma\" OR ato OR bolsonarista OR golpe OR golpista OR baderna OR extremista OR Brasília OR \"três poderes\" OR invasão OR \"ocupar congresso\" OR \"atos terroristas\" OR manifestção OR atentado OR patriotas OR \"tomada do poder\" OR guerra OR \"esplanada dos ministérios\" OR \"congresso nacional\" OR \"manifestantes\" OR \"retomada do poder\") -ucrania lang:pt ')\n",
    "\n",
    "_env.set(\"retweets_numero_bolsonaro\", '22 lang:pt is:retweet', False)\n",
    "\n",
    "_env.set(\"retweets_numero_lula\",'13 lang:pt is:retweet', False)\n",
    "\n",
    "_env.set(\"retweets_pos_eleicao\", 'is:retweet (\"intervenção militar\" OR \"intervenção federal\" OR \"alexandre de morais\" OR xandão OR fraude OR Venezuela OR Cuba OR urna OR urnas OR comunismo) lang:pt', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordem = (\"query_bolsonaro\", \"query_lula\", \"query_ciro\", \"query_simone\", \"perfil_bolsonaro\", \"perfil_lula\", \"perfil_ciro\", \"perfil_simone\", \"numero_bolsonaro\", \"numero_lula\", 'pos_eleicao','atos_golpistas')\n",
    "perfis = (\"tweets_bolsonaro\", \"tweets_lula\", \"tweets_ciro\", \"tweets_simone\")\n",
    "retweets_queries = (\"retweets_query_bolsonaro\", \"retweets_query_lula\", \"retweets_query_ciro\", \"retweets_query_simone\", 'retweets_atos_golpistas', 'retweets_numero_bolsonaro', 'retweets_numero_lula', 'retweets_pos_eleicao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_law(query: str, date: datetime) -> bool:\n",
    "    global ordem, perfis, retweets_queries\n",
    "\n",
    "    number_queires = (\"numero_bolsonaro\",\"numero_lula\",\"retweets_numero_bolsonaro\",\"retweets_numero_lula\")\n",
    "    pos_eleicao = (\"pos_eleicao\", 'retweets_pos_eleicao')\n",
    "    atos_golpistas = ('atos_golpistas', 'retweets_atos_golpistas')\n",
    "    \n",
    "    return (\n",
    "        (\n",
    "            query in number_queires\n",
    "            and date > datetime(year= 2022, month= 10, day= 1) \n",
    "            and date < datetime(year= 2022, month= 10, day= 31)\n",
    "        ) or \n",
    "        ((query in ordem[:8] or query in retweets_queries[:4]) and date < datetime(year= 2023, month= 2, day= 1)) or\n",
    "        (query in pos_eleicao and date > datetime(year= 2022, month= 10, day= 30) and date <= datetime(year= 2023, month= 2, day= 24)) or\n",
    "        (query in atos_golpistas and date >= datetime(year= 2022, month= 12, day= 31) and date <= datetime(year= 2023, month= 2, day= 12))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_law('retweets_lula', datetime(year= 2023, month= 1, day= 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contagem(date: datetime):\n",
    "    global ordem, retweets_queries\n",
    "    \n",
    "    tweets_count = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    retweets_count = [0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    while date > datetime(year= 2022, month= 7, day= 19):\n",
    "        print(f'Dia: {date}')\n",
    "\n",
    "        for index, query in enumerate(ordem):\n",
    "            try:\n",
    "                coleta_em_andamento = __simplify_query(_env.get(query, generate_if_missing = False)) + date.strftime(\"%d%m%Y\")\n",
    "\n",
    "                status_info = _env.get(coleta_em_andamento, generate_if_missing = False)\n",
    "                \n",
    "                df = pd.read_csv(status_info['csv_path'] + '_tweets.csv', sep=';', escapechar= '\\\\', dtype= _dtype)\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "\n",
    "                counts = df[\"type\"].value_counts()\n",
    "\n",
    "                try:\n",
    "                    counts = counts.drop(\"retweeted\")\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    counts = counts.drop(\"type\")\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "                for i in counts:\n",
    "                    tweets_count[index] += i\n",
    "                    tweets_count[-1] += i\n",
    "\n",
    "\n",
    "        for index, query in enumerate(retweets_queries):\n",
    "            try:\n",
    "                coleta_em_andamento = __simplify_query(_env.get(query, generate_if_missing = False)) + date.strftime(\"%d%m%Y\")\n",
    "\n",
    "                status_info = _env.get(coleta_em_andamento, generate_if_missing = False)\n",
    "                \n",
    "                df = pd.read_csv(status_info['csv_path'] + '_tweets.csv', sep=';', escapechar= '\\\\', dtype= _dtype)\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "\n",
    "                counts = df[\"type\"].value_counts()\n",
    "\n",
    "                for i in counts:\n",
    "                    retweets_count[index] += i\n",
    "                    retweets_count[-1] += i\n",
    "\n",
    "\n",
    "        date -= timedelta(days= 1)\n",
    "\n",
    "    contagem = try_loadfrom_drive(open)('contagem.txt', 'w')\n",
    "    for i, query in enumerate(ordem):\n",
    "        contagem.write(f\"{query}: {tweets_count[i]:,} tweets coletados\\n\")\n",
    "\n",
    "    contagem.write(f\"total tweets: {tweets_count[-1]:,} tweets coletados\\n\")\n",
    "    for i, query in enumerate(retweets_queries):\n",
    "        contagem.write(f\"{query}: {retweets_count[i]:,} retweets coletados\\n\")\n",
    "\n",
    "    contagem.write(f\"total retweets: {retweets_count[-1]:,} retweets coletados\\n\")\n",
    "    contagem.write(f\"total: {tweets_count[-1] + retweets_count[-1]:,} tweets and retweets coletados\\n\")\n",
    "\n",
    "    contagem.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coleta_diaria(date: datetime):\n",
    "    global ordem\n",
    "    \n",
    "    date_aux = date\n",
    "\n",
    "    while date > datetime(year= 2022, month= 7, day= 20):\n",
    "            \n",
    "        thread = 'coleta'\n",
    "        for query in ordem:\n",
    "            if (query_law(query,date)):        \n",
    "                print(f\"thread {(thread.upper()+':'):<12}{query}\")\n",
    "                collect_day_tweets_from_date(_env.get(query), query, date - timedelta(days= 1), thread)\n",
    "\n",
    "        thread = 'superior'\n",
    "        for query in ordem:\n",
    "            print(f\"thread {(thread.upper()+':'):<12}{query}\")\n",
    "            get_rede_ativa_superior(_env.get(query), query, date - timedelta(days= 1), thread)\n",
    "\n",
    "        date -= timedelta(days= 1)\n",
    "\n",
    "    contagem(date_aux)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retweets(query: list):\n",
    "\n",
    "    date = datetime(year= 2022, month= 7, day= 20)\n",
    "\n",
    "    while date < (datetime(datetime.now().year, datetime.now().month, datetime.now().day) - timedelta(days= 1)):\n",
    "            \n",
    "        thread = 'retweets'\n",
    "        \n",
    "        for q in query:\n",
    "            if (query_law(q,date)):        \n",
    "                print(f\"thread {(thread.upper()+':'):<12}{q}\")\n",
    "                collect_day_tweets_from_date(_env.get(q), q, date, thread)\n",
    "\n",
    "        date += timedelta(days= 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likes():\n",
    "    global ordem\n",
    "    \n",
    "    date = datetime(year= 2022, month= 7, day= 20)\n",
    "\n",
    "\n",
    "    while date < datetime(datetime.now().year, datetime.now().month, datetime.now().day):\n",
    "        \n",
    "        thread = 'likes'\n",
    "        for query in ordem:\n",
    "            if (query_law(query,date)):\n",
    "                print(f\"thread {(thread.upper()+':'):<12}{query}\")\n",
    "                get_likes(_env.get(query), date, thread)\n",
    "\n",
    "        date += timedelta(days= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quotesReplies():\n",
    "    global perfis\n",
    "    \n",
    "    date = datetime(year= 2022, month= 7, day= 20)\n",
    "\n",
    "\n",
    "    while date < datetime(datetime.now().year, datetime.now().month, datetime.now().day):\n",
    "        \n",
    "        thread = 'quotes'\n",
    "        for query in perfis:\n",
    "            print(f\"thread {(thread.upper()+':'):<12}{query}\")\n",
    "            get_quotes(_env.get(query), date, thread)\n",
    "\n",
    "        thread = 'replies'\n",
    "        for query in perfis:\n",
    "            print(f\"thread {(thread.upper()+':'):<12}{query}\")\n",
    "            get_replies(_env.get(query), date, thread)\n",
    "\n",
    "        date += timedelta(days= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    threads = []\n",
    "\n",
    "    threads.append(Thread(target= retweets))\n",
    "    #threads.append(Thread(target= likes))\n",
    "    threads.append(Thread(target= quotesReplies))\n",
    "    \n",
    "    for t in threads:\n",
    "        t.start()\n",
    "\n",
    "    today = datetime(datetime.now().year, datetime.now().month, datetime.now().day) - timedelta(days= 1)\n",
    "\n",
    "    while True:\n",
    "        if today < datetime(datetime.now().year, datetime.now().month, datetime.now().day):\n",
    "            today = datetime(datetime.now().year, datetime.now().month, datetime.now().day)\n",
    "            #coleta_diaria(today)\n",
    "        sleep(60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuildJSON():\n",
    "    global ordem, _env, _workspace\n",
    "\n",
    "\n",
    "    date = datetime(year= 2022, month= 7, day= 20)\n",
    "    while date < datetime(datetime.now().year, datetime.now().month, datetime.now().day):\n",
    "\n",
    "        for q in (ordem + retweets_queries):\n",
    "            if (query_law(q, date)):\n",
    "                query = _env.get(q, generate_if_missing = False)\n",
    "                directory = _workspace + '/Eleicoes_2022_Pesquisa/coleta/hashtags/'+ q +'/'\n",
    "                directory += date.strftime(\"%Y/%m/%d\")\n",
    "\n",
    "                try:\n",
    "                    tweets_file_dir = listdir(directory)\n",
    "                except:\n",
    "                    pass\n",
    "                else:\n",
    "                    #csv_path\n",
    "                    csv_path = directory + '/' + tweets_file_dir[-2][:tweets_file_dir[-2].rfind('_tweets')]\n",
    "                    \n",
    "                    status_info = {\n",
    "                        \"csv_path\": csv_path,\n",
    "                        \"next_page_token\": 'fim',\n",
    "                        \"rede_ativa_superior\": True,\n",
    "                    }\n",
    "\n",
    "                    for thread in ('quotes', 'replies'):\n",
    "                        if thread in tweets_file_dir:\n",
    "                            status_info.update({\n",
    "                                thread: {\n",
    "                                    \"index\": 'fim',\n",
    "                                    \"next_page_token\": None,\n",
    "                                }\n",
    "                            })\n",
    "                        #\n",
    "                    #\n",
    "\n",
    "                    coleta_em_andamento = q + date.strftime(\"%d%m%Y\")\n",
    "                    _env.set(coleta_em_andamento, status_info, True)\n",
    "                #\n",
    "            #\n",
    "        #\n",
    "        date += timedelta(days= 1)\n",
    "    #\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testeJSON():\n",
    "    date = datetime(year= 2022, month= 7, day= 20)\n",
    "    \n",
    "    while date < datetime(datetime.now().year, datetime.now().month, datetime.now().day):\n",
    "\n",
    "        for q in (ordem + retweets_queries):\n",
    "            if (query_law(q, date)):\n",
    "                query = _env.get(q, generate_if_missing = False)\n",
    "                coleta_em_andamento = q + date.strftime(\"%d%m%Y\")\n",
    "\n",
    "                status_info = _env.get(coleta_em_andamento, generate_if_missing = False)\n",
    "\n",
    "                print(f'{status_info != False}: {q:>10}_{date.strftime(\"%d/%m/%Y\"):>10}')\n",
    "            #\n",
    "        #\n",
    "        date += timedelta(days= 1)\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(date: datetime, query_key: str):\n",
    "    global _env\n",
    "\n",
    "    query = _env.get(query_key, generate_if_missing = False)\n",
    "    coleta_em_andamento = __simplify_query(query) + date.strftime(\"%d%m%Y\")\n",
    "    status_info = _env.get(coleta_em_andamento, generate_if_missing = False)\n",
    "    \n",
    "    return pd.read_csv(status_info['csv_path'] + '_tweets.csv', sep=';', escapechar= '\\\\', dtype= _dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dataframe(df: pd.DataFrame, date: datetime, query_key: str):\n",
    "    global _env\n",
    "\n",
    "    query = _env.get(query_key, generate_if_missing = False)\n",
    "    coleta_em_andamento = __simplify_query(query) + date.strftime(\"%d%m%Y\")\n",
    "    status_info = _env.get(coleta_em_andamento, generate_if_missing = False)\n",
    "\n",
    "    df.to_csv(status_info['csv_path'] + '_tweets.csv', sep= ';', escapechar= '\\\\', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   retweets_query_lula\n",
      "thread RETWEETS:   retweets_pos_eleicao\n",
      "thread RETWEETS:   retweets_query_bolsonaro\n",
      "thread RETWEETS:   403 Forbidden\n",
      "When authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.\n",
      "thread RETWEETS:   BACKUP INICIADO\n",
      "thread RETWEETS:   BACKUP FINALIZADO\n",
      "thread RETWEETS:   403 Forbidden\n",
      "When authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.\n",
      "thread RETWEETS:   403 Forbidden\n",
      "When authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jimi/Área de Trabalho/ColetaTweets/Coleta_ Atualizada_v3.ipynb Célula 85\u001b[0m in \u001b[0;36mcollect_day_tweets_from_date\u001b[0;34m(query, query_name, date, thread)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     response \u001b[39m=\u001b[39m _client[thread]\u001b[39m.\u001b[39;49msearch_all_tweets(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         query\u001b[39m=\u001b[39;49m query,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         user_fields\u001b[39m=\u001b[39;49m _user_fields,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         tweet_fields\u001b[39m=\u001b[39;49m _tweet_fields,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         media_fields\u001b[39m=\u001b[39;49m _media_fields,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         expansions\u001b[39m=\u001b[39;49m _expansions,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         max_results\u001b[39m=\u001b[39;49m \u001b[39m500\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         next_token\u001b[39m=\u001b[39;49m status_info[\u001b[39m\"\u001b[39;49m\u001b[39mnext_page_token\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         end_time\u001b[39m=\u001b[39;49m date,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         start_time\u001b[39m=\u001b[39;49m date \u001b[39m-\u001b[39;49m timedelta(days\u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mexcept\u001b[39;00m tweepy\u001b[39m.\u001b[39mTweepyException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tweepy/client.py:1145\u001b[0m, in \u001b[0;36mClient.search_all_tweets\u001b[0;34m(self, query, **params)\u001b[0m\n\u001b[1;32m   1144\u001b[0m params[\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m query\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m   1146\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m/2/tweets/search/all\u001b[39;49m\u001b[39m\"\u001b[39;49m, params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m   1147\u001b[0m     endpoint_parameters\u001b[39m=\u001b[39;49m(\n\u001b[1;32m   1148\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mend_time\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mexpansions\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmax_results\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmedia.fields\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1149\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mnext_token\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mplace.fields\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mpoll.fields\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1150\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39msince_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msort_order\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstart_time\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtweet.fields\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1151\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39muntil_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39muser.fields\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m   1152\u001b[0m     ), data_type\u001b[39m=\u001b[39;49mTweet\n\u001b[1;32m   1153\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tweepy/client.py:126\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[0;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[1;32m    124\u001b[0m request_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_params(params, endpoint_parameters)\n\u001b[0;32m--> 126\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(method, route, params\u001b[39m=\u001b[39;49mrequest_params,\n\u001b[1;32m    127\u001b[0m                         json\u001b[39m=\u001b[39;49mjson, user_auth\u001b[39m=\u001b[39;49muser_auth)\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_type \u001b[39mis\u001b[39;00m requests\u001b[39m.\u001b[39mResponse:\n",
      "\u001b[1;32m/home/jimi/Área de Trabalho/ColetaTweets/Coleta_ Atualizada_v3.ipynb Célula 85\u001b[0m in \u001b[0;36mtokenFarmClient.request\u001b[0;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrequest(method, route, params, json, user_auth)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   response_ok \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tweepy/client.py:99\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[0;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m403\u001b[39m:\n\u001b[0;32m---> 99\u001b[0m     \u001b[39mraise\u001b[39;00m Forbidden(response)\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m404\u001b[39m:\n",
      "\u001b[0;31mForbidden\u001b[0m: 403 Forbidden\nWhen authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jimi/Área de Trabalho/ColetaTweets/Coleta_ Atualizada_v3.ipynb Célula 85\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mPYTHONWARNINGS\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m# Also affect subprocesses\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#rebuildJSON()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#testeJSON()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#coleta_diaria(datetime(datetime.now().year, datetime.now().month, datetime.now().day) - timedelta(days= 1))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m retweets([\u001b[39m'\u001b[39;49m\u001b[39mretweets_query_bolsonaro\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mretweets_query_lula\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mretweets_pos_eleicao\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[1;32m/home/jimi/Área de Trabalho/ColetaTweets/Coleta_ Atualizada_v3.ipynb Célula 85\u001b[0m in \u001b[0;36mretweets\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m (query_law(q,date)):        \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthread \u001b[39m\u001b[39m{\u001b[39;00m(thread\u001b[39m.\u001b[39mupper()\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m:\u001b[39;00m\u001b[39m<12\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mq\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         collect_day_tweets_from_date(_env\u001b[39m.\u001b[39;49mget(q), q, date, thread)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m date \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m timedelta(days\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/jimi/Área de Trabalho/ColetaTweets/Coleta_ Atualizada_v3.ipynb Célula 85\u001b[0m in \u001b[0;36mcollect_day_tweets_from_date\u001b[0;34m(query, query_name, date, thread)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m         backup_state(coleta_em_andamento, status_info, thread)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         backup \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m   \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     sleep(\u001b[39m5\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e: \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jimi/%C3%81rea%20de%20Trabalho/ColetaTweets/Coleta_%20Atualizada_v3.ipynb#Y150sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthread \u001b[39m\u001b[39m{\u001b[39;00m(thread\u001b[39m.\u001b[39mupper()\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m:\u001b[39;00m\u001b[39m<12\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import os, warnings\n",
    "    warnings.simplefilter(\"ignore\") # Change the filter in this process\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"default\" # Also affect subprocesses\n",
    "\n",
    "#rebuildJSON()\n",
    "#testeJSON()\n",
    "\n",
    "#coleta_diaria(datetime(datetime.now().year, datetime.now().month, datetime.now().day) - timedelta(days= 1))\n",
    "retweets(['retweets_query_bolsonaro', 'retweets_query_lula', 'retweets_pos_eleicao'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6627370021ca0f43ab6e103b6d8b2524bfb73d9ea0ba57b6ea6b68b7cb689139"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
